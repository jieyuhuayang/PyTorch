{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: 自动求导(automatic differentiation)\n",
    "PyTorch中，所有神经网络的核心是autograd包。     \n",
    "autograd包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义（define-by-run）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的。  \n",
    "  \n",
    "## 1. tensor  \n",
    "torch.Tensor是这个包的核心类。  \n",
    "如果设置它的属性 ```.requires_grad```为True，那么它将会追踪对于该张量的所有操作。   \n",
    "当完成计算后可以通过调用```.backward()```，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到```.grad```属性。  \n",
    "  \n",
    "要阻止一个张量被跟踪历史，可以调用.detach()方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。  \n",
    "  \n",
    "为了防止跟踪历史记录（和使用内存），可以将代码块包装在```with torch.no_grad():```中。在评估模型时特别有用，因为模型可能具有```requires_grad = True```的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
