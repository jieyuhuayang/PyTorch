{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面我们已经介绍了```autograd```包，现在我们使用```torch.nn```包来构建神经网络，```nn```包依赖于```autograd```包来定义模型并对它们求导。    \n",
    "```nn.Module```是神经网络模块。包含神经网络各个层和一个```forward(input)```方法，该方法返回```output```。    \n",
    "```nn.Parameter```也是一个tensor，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Torch 中的激励函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFpCAYAAAC1YKAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 576x432 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "# 做一些假数据来观看图像\n",
    "x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1)\n",
    "x = Variable(x) \n",
    "\n",
    "x_np = x.numpy()   # 换成 numpy array, 出图时用  \n",
    "\n",
    "# 几种常用的 激励函数\n",
    "y_relu = torch.relu(x).detach().numpy()\n",
    "y_sigmoid = torch.sigmoid(x).detach().numpy()\n",
    "y_tanh = torch.tanh(x).detach().numpy()\n",
    "\n",
    "plt.figure(1, figsize=(8, 6))\n",
    "plt.subplot(221)\n",
    "plt.plot(x_np, y_relu, c='red', label='relu')\n",
    "plt.ylim((-1, 5))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(x_np, y_sigmoid, c='red', label='sigmoid')\n",
    "plt.ylim((-0.2, 1.2))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.plot(x_np, y_tanh, c='red', label='tanh')\n",
    "plt.ylim((-1.2, 1.2))\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. torch.nn中的网络结构组件\n",
    "## 2.1 二维卷积层\n",
    "- nn.Conv2d将接受一个4维的张量：```nSamples×nChannels×Height×Width```,输出一个4维张量：```nSamples×nChannels\\_out×Height\\_out×Width\\_out```\n",
    "- ```torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)```  \n",
    "  \n",
    "其他网络结构详见nn包文档：[https://pytorch.org/docs/stable/nn.html#](https://pytorch.org/docs/stable/nn.html#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. torch.nn中的损失函数 \n",
    "一个损失函数接受一对(output, target)作为输入，计算一个值来估计网络的输出和目标值相差多少。  \n",
    "详见nn包文档：[https://pytorch.org/docs/stable/nn.html#](https://pytorch.org/docs/stable/nn.html#)\n",
    "## 3.1 reduction参数\n",
    "- PyTorch 0.4版本后，使用reduction参数控制损失函数的输出行为。  \n",
    "    - reduction (string, optional)   \n",
    "        - ‘none’: 不进行数据降维，输出为向量  \n",
    "        - ‘mean’： 将向量中的数累加求和，然后除以元素数量，返回误差的平均值  \n",
    "        - ‘sum’： 返回向量的误差值的和  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 定义一个网络  \n",
    "网络上的很多代码都是PyTorch0.4以前的版本，自从0.4后variable和tensor合并，tensor也具有requires_grad的属性了。  \n",
    "## 4.1 定义网络架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # CLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # view函数将张量x变形成一维的向量形式，总特征数并不改变，为接下来的全连接作准备。\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    # num_flat_features：计算张量x的总特征量（把每个数字都看出是一个特征，即特征总量）\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension(批大小维度)\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 检查一下参数、输入输出的形状\n",
    "可以看出来，我们设置了5层网络，得到了10个参数（（权重+偏置）* 5 层）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "第0个参数形状为torch.Size([6, 1, 5, 5])\n",
      "第1个参数形状为torch.Size([6])\n",
      "第2个参数形状为torch.Size([16, 6, 5, 5])\n",
      "第3个参数形状为torch.Size([16])\n",
      "第4个参数形状为torch.Size([120, 400])\n",
      "第5个参数形状为torch.Size([120])\n",
      "第6个参数形状为torch.Size([84, 120])\n",
      "第7个参数形状为torch.Size([84])\n",
      "第8个参数形状为torch.Size([10, 84])\n",
      "第9个参数形状为torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(type(params[0]))\n",
    "for i in range(10):\n",
    "    print(\"第{}个参数形状为{}\".format(i,params[i].size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个网络（LeNet）的期待输入是32x32，让我们随机生成一组数据作为输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1输出的形状： torch.Size([1, 6, 28, 28])\n",
      "输出层形状 torch.Size([1, 10])\n",
      "输出层结果： tensor([[ 0.1090,  0.0137,  0.0531, -0.0406, -0.0739,  0.0899,  0.0254,  0.0115,\n",
      "          0.0654,  0.0346]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "print('conv1输出的形状：',net.conv1(input).size()) # conv1输出的形状\n",
    "\n",
    "out = net(input)\n",
    "print('输出层形状',out.size())\n",
    "print('输出层结果：',out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 定义损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: tensor(0.6886, grad_fn=<MseLossBackward>)\n",
      "\n",
      "反向跟踪loss,使用它的.grad_fn属性,你会看到向下面这样的一个计算图:\n",
      "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss\n",
      "<MseLossBackward object at 0x0000021708CC6710>\n",
      "<AddmmBackward object at 0x000002170910CE10>\n",
      "<AccumulateGrad object at 0x0000021708CC6710>\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss() \n",
    "\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print('loss:',loss)\n",
    "print('''\n",
    "反向跟踪loss,使用它的.grad_fn属性,你会看到向下面这样的一个计算图:\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d -> view -> linear -> relu -> linear -> relu -> linear -> MSELoss -> loss''')\n",
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包torch.optim 实现了各种不同的参数更新规则，如SGD、Nesterov-SGD、Adam、RMSProp等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 训练神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6886, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6061, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5165, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3901, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2053, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0425, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0061, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    output = net(input)\n",
    "    loss = criterion(output, target)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别用numpy和PyTorch实现一个简单的梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: -0.2 y: 0.64\n",
      "x: -0.36000000000000004 y: 0.40959999999999996\n",
      "x: -0.488 y: 0.26214400000000004\n",
      "x: -0.5904 y: 0.16777215999999995\n",
      "x: -0.67232 y: 0.10737418239999996\n",
      "x: -0.7378560000000001 y: 0.06871947673599998\n",
      "x: -0.7902848 y: 0.043980465111040035\n",
      "x: -0.83222784 y: 0.028147497671065613\n",
      "x: -0.865782272 y: 0.018014398509481944\n",
      "x: -0.8926258176 y: 0.011529215046068408\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "# 不使用PyTorch做一个简单的梯度下降\n",
    "#######################\n",
    "x = 0 \n",
    "\n",
    "# 学习率\n",
    "learning_rate= 0.1 \n",
    "\n",
    "# 迭代次数\n",
    "epochs = 10\n",
    "\n",
    "# lambda函数定义一个简单的函数，假装是在算loss :)\n",
    "y = lambda x: x**2 + 2*x +1 \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    dx = 2*x +2 #梯度\n",
    "    x = x - learning_rate*dx #在梯度上进行更新\n",
    "    print('x:',x,'y:',y(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= tensor([0.]) y= tensor([1.])\n",
      "x= tensor([-0.2000]) y= tensor([0.6400])\n",
      "x= tensor([-0.3600]) y= tensor([0.4096])\n",
      "x= tensor([-0.4880]) y= tensor([0.2621])\n",
      "x= tensor([-0.5904]) y= tensor([0.1678])\n",
      "x= tensor([-0.6723]) y= tensor([0.1074])\n",
      "x= tensor([-0.7379]) y= tensor([0.0687])\n",
      "x= tensor([-0.7903]) y= tensor([0.0440])\n",
      "x= tensor([-0.8322]) y= tensor([0.0281])\n",
      "x= tensor([-0.8658]) y= tensor([0.0180])\n",
      "x= tensor([-0.8926]) y= tensor([0.0115])\n",
      "x= tensor([-0.9141]) y= tensor([0.0074])\n",
      "x= tensor([-0.9313]) y= tensor([0.0047])\n",
      "x= tensor([-0.9450]) y= tensor([0.0030])\n",
      "x= tensor([-0.9560]) y= tensor([0.0019])\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "# PyTorch 实现一个简单的梯度下降\n",
    "#################\n",
    "import torch\n",
    "\n",
    "# 设定初始值\n",
    "x = torch.Tensor([0])\n",
    "x.requires_grad_(True)\n",
    "\n",
    "# 自己定义一个函数，假装是在算loss好了\n",
    "y= x**2 + 2*x + 1\n",
    "learning_rate=torch.Tensor([0.1])\n",
    "epoches = 15\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    y= x**2 + 2*x + 1\n",
    "    # 反向传播求梯度\n",
    "    y.backward()\n",
    "    print('x=',x.data,'y=',y.data)\n",
    "\n",
    "    x.data= x.data - learning_rate*x.grad.data\n",
    "    # 在 PyTorch 中梯度如果不清零会积累((因为PyTorch是基于动态图的, 每迭代一次就会留下计算缓存, 到一下次循环时需要手动清楚缓存))\n",
    "    x.grad.data.zero_()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**利用PyTorch实现一个简单的线性回归**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 epoch's loss: 40.381011962890625\n",
      "1 epoch's loss: 0.6335387229919434\n",
      "2 epoch's loss: 0.15156620740890503\n",
      "3 epoch's loss: 0.1389687955379486\n",
      "4 epoch's loss: 0.13230319321155548\n",
      "5 epoch's loss: 0.1260179728269577\n",
      "6 epoch's loss: 0.12003206461668015\n",
      "7 epoch's loss: 0.11433039605617523\n",
      "8 epoch's loss: 0.10889953374862671\n",
      "9 epoch's loss: 0.10372686386108398\n",
      "10 epoch's loss: 0.09879972040653229\n",
      "11 epoch's loss: 0.09410669654607773\n",
      "12 epoch's loss: 0.08963649719953537\n",
      "13 epoch's loss: 0.08537869900465012\n",
      "14 epoch's loss: 0.08132310211658478\n",
      "15 epoch's loss: 0.0774601548910141\n",
      "16 epoch's loss: 0.07378076016902924\n",
      "17 epoch's loss: 0.07027611881494522\n",
      "18 epoch's loss: 0.06693806499242783\n",
      "19 epoch's loss: 0.06375843286514282\n",
      "20 epoch's loss: 0.06072979047894478\n",
      "21 epoch's loss: 0.05784515291452408\n",
      "22 epoch's loss: 0.05509743094444275\n",
      "23 epoch's loss: 0.05248021334409714\n",
      "24 epoch's loss: 0.04998737946152687\n",
      "25 epoch's loss: 0.04761295020580292\n",
      "26 epoch's loss: 0.04535127803683281\n",
      "27 epoch's loss: 0.04319709539413452\n",
      "28 epoch's loss: 0.041145194321870804\n",
      "29 epoch's loss: 0.039190784096717834\n",
      "30 epoch's loss: 0.037329185754060745\n",
      "31 epoch's loss: 0.03555605560541153\n",
      "32 epoch's loss: 0.03386708348989487\n",
      "33 epoch's loss: 0.03225836530327797\n",
      "34 epoch's loss: 0.03072609379887581\n",
      "35 epoch's loss: 0.0292666032910347\n",
      "36 epoch's loss: 0.02787637896835804\n",
      "37 epoch's loss: 0.02655218541622162\n",
      "38 epoch's loss: 0.025290999561548233\n",
      "39 epoch's loss: 0.024089643731713295\n",
      "the result of y when x is 4: tensor([7.6962], grad_fn=<AddBackward0>)\n",
      "model.parameter: [Parameter containing:\n",
      "tensor([[1.8241]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3999], requires_grad=True)]\n",
      "<class 'list'>\n",
      "Parameter containing:\n",
      "tensor([0.3999], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "#print(torch.__version__)\n",
    "\n",
    "# train data \n",
    "x_data= torch.arange(1.0,4.0,1.0)\n",
    "x_data=x_data.view(-1,1)\n",
    "y_data= torch.arange(2.0,7.0,2.0)\n",
    "y_data= y_data.view(-1,1)\n",
    "\n",
    "# 超参数设置\n",
    "learning_rate=0.1\n",
    "num_epoches=40\n",
    "\n",
    "# 线性回归模型\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression,self).__init__()\n",
    "        self.linear = torch.nn.Linear(1,1)# 1 in and 1 out\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# 定义loss function损失函数和optimizer优化器\n",
    "# PyTorch0.4以后，使用reduction参数控制损失函数的输出行为\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "# nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epoches):\n",
    "    # forward \n",
    "    y_pred= model(x_data)\n",
    "    \n",
    "    #computing loss \n",
    "    loss = criterion(y_pred,y_data)\n",
    "    \n",
    "    print(epoch,'epoch\\'s loss:',loss.item())\n",
    "    \n",
    "    # backward: zero gradients + backward + step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()  \n",
    "    optimizer.step() # 执行一步-梯度下降（1-step gradient descent）\n",
    "    \n",
    "# testing\n",
    "x_test=torch.Tensor([4.0])\n",
    "print(\"the result of y when x is 4:\",model(x_test))\n",
    "print('model.parameter:',list(model.parameters()))\n",
    "#print(type(list(model.parameters())))\n",
    "#print(list(model.parameters())[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
