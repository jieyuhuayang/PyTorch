{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 张量（Tensor)\n",
    "tensor于numpy的ndarrays，不同之处在于tensor可以使用GPU来加快计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 创建一个未初始化的5*3的矩阵："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  1.2163e-42],\n",
      "        [ 0.0000e+00,  7.0065e-45,  0.0000e+00],\n",
      "        [-2.6090e+09,  5.0587e-43,  0.0000e+00],\n",
      "        [ 0.0000e+00,  7.0065e-45,  0.0000e+00],\n",
      "        [-3.0255e+09,  5.0587e-43,  0.0000e+00]])\n",
      "x.dtype= torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor(5, 3)\n",
    "print(x)\n",
    "print('x.dtype=',x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 获取tensor的属性（形状大小、维度个数、数据类型、元素个数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "法①：x.size()= torch.Size([5, 3])\n",
      "torch.Size()返回的实际上是一个元组，所以它支持元组的所有操作。\n",
      "法②：x.shape= torch.Size([5, 3])\n",
      "\n",
      "x.dim()= 2\n",
      "\n",
      "x.dtype= torch.float32\n",
      "\n",
      "x.numel()= 15\n"
     ]
    }
   ],
   "source": [
    "# 获取形状大小的两种方法：\n",
    "print('法①：x.size()=',x.size())\n",
    "print('torch.Size()返回的实际上是一个元组，所以它支持元组的所有操作。')\n",
    "print('法②：x.shape=',x.shape)\n",
    "print()\n",
    "\n",
    "# 获取维度个数\n",
    "print('x.dim()=',x.dim())\n",
    "print()\n",
    "\n",
    "# 获取数据类型\n",
    "print('x.dtype=',x.dtype)\n",
    "print()\n",
    "\n",
    "# 获取元素个数\n",
    "print('x.numel()=',x.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 构建一个初始化矩阵\n",
    "- 0初始化，使用long的类型  \n",
    "- 1初始化\n",
    "- 随机[0, 1)初始化\n",
    "- 等差数列初始化 \n",
    "- 标准正态分布初始化\n",
    "- eye初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 12, 14, 16, 18])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "# 1初始化\n",
    "# x = torch.ones(5, 3, dtype=torch.long)\n",
    "\n",
    "# [0,1)初始化\n",
    "# x = torch.rand(5,3)\n",
    "\n",
    "# 等差数列初始化 \n",
    "x = torch.arange(10,20,2)\n",
    "\n",
    "# 标准正态分布初始化\n",
    "# x =torch.randn(5,3)\n",
    "\n",
    "# eye初始化：para1决定张量的形状，比如3，就是三行三列，para2决定张量输出的列数，比如2，输出列数则为2。\n",
    "# x=torch.eye(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 从数据中直接构建一个tensor："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x,x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 根据已有的tensor建立新的tensor:\n",
    "除非用户提供新值,这些方法将重用输入张量(tensor)的属性，例如dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.7542, -0.1806, -0.7456],\n",
      "        [-0.3787,  1.2365,  1.0395],\n",
      "        [ 0.8733, -2.2402, -0.8755],\n",
      "        [ 0.9496,  1.5125,  0.0083],\n",
      "        [-0.5588, -0.5435,  0.0065]])\n"
     ]
    }
   ],
   "source": [
    "x = x.new_zeros(5, 3, dtype=torch.double)     # 打印：tensor([[0,0,0],[0,0,0],...])\n",
    "print(x)\n",
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # 覆盖类型! result 的size相同，数据变为随机值。\n",
    "print(x)                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. tensor上的运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种运算有多种语法。在下面的示例中，我们将研究加法运算。\n",
    "   ## 2.1 加法的3种形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6529,  0.4947,  0.2033],\n",
      "        [ 0.5041,  1.5942,  1.9702],\n",
      "        [ 1.5319, -1.7636,  0.0310],\n",
      "        [ 1.6111,  1.5339,  0.7098],\n",
      "        [ 0.4113, -0.5045,  0.7432]])\n",
      "tensor([[ 1.6529,  0.4947,  0.2033],\n",
      "        [ 0.5041,  1.5942,  1.9702],\n",
      "        [ 1.5319, -1.7636,  0.0310],\n",
      "        [ 1.6111,  1.5339,  0.7098],\n",
      "        [ 0.4113, -0.5045,  0.7432]])\n",
      "tensor([[ 1.6529,  0.4947,  0.2033],\n",
      "        [ 0.5041,  1.5942,  1.9702],\n",
      "        [ 1.5319, -1.7636,  0.0310],\n",
      "        [ 1.6111,  1.5339,  0.7098],\n",
      "        [ 0.4113, -0.5045,  0.7432]])\n"
     ]
    }
   ],
   "source": [
    "# 加法的2种形式\n",
    "# 法①\n",
    "y = torch.rand(5, 3)\n",
    "c=x + y\n",
    "print(c)\n",
    "\n",
    "# 法②\n",
    "print(torch.add(x, y))\n",
    "\n",
    "# 法③\n",
    "result = torch.empty(5, 2)\n",
    "#print(result)\n",
    "torch.add(x,y,out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 原地操作（in-place）\n",
    ">注意：\n",
    ">任何一个改变张量的操作后面都固定一个\\_。例如x.copy\\_(y)、x.t\\_()将更改x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6529,  0.4947,  0.2033],\n",
      "        [ 0.5041,  1.5942,  1.9702],\n",
      "        [ 1.5319, -1.7636,  0.0310],\n",
      "        [ 1.6111,  1.5339,  0.7098],\n",
      "        [ 0.4113, -0.5045,  0.7432]])\n"
     ]
    }
   ],
   "source": [
    "# 把x加到y上\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 可以使用像标准的NumPy一样的各种索引操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7542, -0.1806, -0.7456],\n",
      "        [-0.3787,  1.2365,  1.0395],\n",
      "        [ 0.8733, -2.2402, -0.8755],\n",
      "        [ 0.9496,  1.5125,  0.0083],\n",
      "        [-0.5588, -0.5435,  0.0065]])\n",
      "tensor([-0.1806,  1.2365, -2.2402,  1.5125, -0.5435])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 改变形状\n",
    "如果想改变形状，可以使用torch.view,还可以使用unsqueeze()函数增加维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "tensor([[ 1.1834, -0.4473, -1.4750, -1.4320],\n",
      "        [ 0.1730,  0.0911, -1.2494,  1.0530],\n",
      "        [ 0.6912,  1.1631, -1.0222,  1.0408],\n",
      "        [ 0.1931, -1.0624, -1.0157,  0.4932]])\n",
      "tensor([ 1.1834, -0.4473, -1.4750, -1.4320,  0.1730,  0.0911, -1.2494,  1.0530,\n",
      "         0.6912,  1.1631, -1.0222,  1.0408,  0.1931, -1.0624, -1.0157,  0.4932])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # -1的意思是没有指定维度\n",
    "print(x.size(), y.size(), z.size())\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4])\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([[[0]],\n",
      "\n",
      "        [[1]],\n",
      "\n",
      "        [[2]],\n",
      "\n",
      "        [[3]],\n",
      "\n",
      "        [[4]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(5)\n",
    "print(a)\n",
    "b=a.unsqueeze(1)\n",
    "print(b)\n",
    "c= b.unsqueeze(1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 如果是仅包含一个元素的tensor，可以使用.item()来得到对应的python数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.8661])\n",
      "-2.866062879562378\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 稍后阅读\n",
    "[这里](https://pytorch.org/docs/stable/torch.html)描述了一百多种张量操作，包括转置，索引，数学运算，线性代数，随机数等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 torch中的数学运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "abs \n",
      "numpy:  [1 2 1 2] \n",
      "torch:  tensor([1., 2., 1., 2.]) torch.float32\n",
      "\n",
      "sin \n",
      "numpy:  [-0.84147098 -0.90929743  0.84147098  0.90929743] \n",
      "torch:  tensor([-0.8415, -0.9093,  0.8415,  0.9093])\n",
      "\n",
      "mean \n",
      "numpy:  0.0 \n",
      "torch:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# abs 绝对值计算\n",
    "data = [-1, -2, 1, 2]\n",
    "print(type(data))\n",
    "tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor\n",
    "print(\n",
    "    '\\nabs',\n",
    "    '\\nnumpy: ', np.abs(data),          # [1 2 1 2]\n",
    "    '\\ntorch: ', torch.abs(tensor),tensor.dtype      # [1 2 1 2]\n",
    ")\n",
    "\n",
    "# sin   三角函数 sin\n",
    "print(\n",
    "    '\\nsin',\n",
    "    '\\nnumpy: ', np.sin(data),      # [-0.84147098 -0.90929743  0.84147098  0.90929743]\n",
    "    '\\ntorch: ', torch.sin(tensor)  # [-0.8415 -0.9093  0.8415  0.9093]\n",
    ")\n",
    "\n",
    "# mean  均值\n",
    "print(\n",
    "    '\\nmean',\n",
    "    '\\nnumpy: ', np.mean(data),         # 0.0\n",
    "    '\\ntorch: ', torch.mean(tensor)     # 0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix multiplication 矩阵点乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "matrix multiplication (matmul) \n",
      "numpy:  [[ 7 10]\n",
      " [15 22]] \n",
      "torch:  tensor([[ 7., 10.],\n",
      "        [15., 22.]])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = [[1,2], [3,4]]\n",
    "print(type(data))\n",
    "tensor = torch.FloatTensor(data)  # 转换成32位浮点 tensor\n",
    "# correct method\n",
    "print(\n",
    "    '\\nmatrix multiplication (matmul)',\n",
    "    '\\nnumpy: ', np.matmul(data, data),     # [[7, 10], [15, 22]]\n",
    "    '\\ntorch: ', torch.mm(tensor, tensor)   # [[7, 10], [15, 22]]\n",
    ")\n",
    "\n",
    "data = np.array(data)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. NumPy桥\n",
    "把一个torch张量转换为numpy数组或者反过来都是很简单的。  \n",
    "Torch张量和NumPy数组将共享它们的底层内存位置，更改一个将更改另一个。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 把Torch张量转换为numpy数组\n",
    "二者共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.]) torch.float32\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "[1. 1. 1. 1. 1.] float32\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a,a.dtype)\n",
    "print(type(a))\n",
    "print()\n",
    "\n",
    "# 把Torch张量a转换为numpy数组b\n",
    "b = a.numpy()\n",
    "print(b,b.dtype)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 把numpy数组转换为torch张量\n",
    "二者共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 CUDA张量\n",
    "可以使用.to方法将张量移动到任何设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. ~~variable~~\n",
    "在 Torch 中的 Variable 就是一个存放会变化的值的变量，里面的值会不停的变化。  \n",
    "  \n",
    "  \n",
    "**PyTorch0.4后Tensor和Variable合并**，torch.Tensor 和torch.autograd.Variable现在是同一个类。torch.Tensor 能够像之前的Variable一样追踪历史和反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n",
      "tensor(7.5000)\n",
      "tensor(7.5000, grad_fn=<MeanBackward1>)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "tensor=torch.FloatTensor([[1,2],[3,4]])\n",
    "variable=Variable(tensor,requires_grad=True)\n",
    "print(tensor*tensor)\n",
    "\n",
    "t_out=torch.mean(tensor*tensor)\n",
    "v_out=torch.mean(variable*variable)\n",
    "print(t_out)\n",
    "print(v_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 variable 计算，梯度  \n",
    "Variable 计算时, 它在背后一步步默默地搭建着计算图, computational graph. 这个图是用来干嘛的? 原来是将所有的计算步骤 (节点) 都连接起来, 最后进行误差反向传递的时候, 一次性将**所有 variable 里面的梯度**都计算出来, 而 tensor 就没有这个能力啦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "v_out.backward()    # 模拟 v_out 的误差反向传递\n",
    "print(variable.grad)    # 初始 Variable 的梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 获取 Variable 里面的数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "<class 'torch.Tensor'>\n",
      "\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#variable形式\n",
    "print(variable)\n",
    "print(type(variable))\n",
    "print()\n",
    "\n",
    "# tensor形式\n",
    "print(variable.data)\n",
    "print(type(variable.data))\n",
    "print()\n",
    "\n",
    "#numpy形式\n",
    "print(variable.detach().numpy())\n",
    "print(type(variable.data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost 算法介绍\n",
    "\n",
    "## 1. Adaboost简介\n",
    "\n",
    "### 1.1 集成学习（ensemble learning）背景介绍\n",
    "\n",
    "集成学习（ensemble learning）通过构建并结合多个学习器（learner）来完成学习任务，通常可获得比单一学习器更良好的泛化性能（特别是在集成弱学习器[weak learner]时）。  \n",
    "\n",
    "目前集成学习主要分为2大类：  \n",
    "\n",
    "一类是以bagging、Random Forest等算法为代表的，**各个学习器之间相互独立、可同时生成**的并行化方法；\n",
    "\n",
    "一类是以boosting、Adaboost等算法为代表的，**个体学习器是串行生成的、具有依赖关系的**序列化方法，它试图不断增强学习器的学习能力。\n",
    "\n",
    "### 1.2 Adaboost背景介绍\n",
    "\n",
    "Adaboost是Yoav Freund和Robert Schapire在1997年提出的一种集成学习（ensemble learning）算法。  \n",
    "\n",
    "此前也Schapire也曾提出了一种boosting算法（Schapire’s Boosting ），但实际应用效果并不好，主要原因是因为Schapire’s Boosting 对于训练集样本的选取条件十分精确而又苛刻，以至于很难找出符合条件的训练集: (。而Adaboost在选取样本的时候采用了基于概率分布（也可解释为基于权值）来选取样本的方法，相比Schapire’s Boosting 放宽了对于训练样本选取的要求。  \n",
    "\n",
    "## 2. Adaboost 算法详解\n",
    "\n",
    "### 2.1 Adaboost 步骤概览\n",
    "\n",
    "1. 初始化训练样本的权值分布，每个训练样本的权值应该相等（如果一共有N个样本，则每个样本的权值为1/N)\n",
    "2. 依次构造训练集并训练弱分类器。如果一个样本被准确分类，那么它的权值在下一个训练集中就会降低；相反，如果它被分类错误，那么它在下个训练集中的权值就会提高。权值更新过后的训练集会用于训练下一个分类器。\n",
    "3. 将训练好的弱分类器集成为一个强分类器，误差率小的弱分类器会在最终的强分类器里占据更大的权重，否则较小。  \n",
    "\n",
    "### 2.2 Adaboost 算法流程\n",
    "\n",
    "给定一个样本数量为m的数据集T=\n",
    "$\n",
    "\\left(x_{1}, y_{1}\\right), \\ldots,\\left(x_{m}, y_{m}\\right)\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
